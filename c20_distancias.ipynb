{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CLASE 19: CÓMO CALCULAR DISTANCIAS ENTRE VECTORES"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Distancia y Norma"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En clases pasadas hablamos de la magnitud de un vector pero no dimos una forma de poder medirla, esa magnitud la podremos calcular a través de la *norma Euclidiana*:\n",
    "\n",
    "$$\n",
    "||x|| = \\displaystyle\\sqrt{x_{0}^2 + x_{1}^2 + \\cdots + x_{n-1}^{2}} = \\displaystyle\\sqrt(x^Tx)\n",
    "$$\n",
    "\n",
    "Hay que notar que $x$ en la expresión anterior es un $n$-vector. \n",
    "Pongamos un ejemplo con $x^T = [1,2,3]$\n",
    "\n",
    "$$\n",
    "||x|| = \\displaystyle\\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14} \\approx 3.74\n",
    "$$\n",
    "\n",
    "Realizaremos otro con $x^T = [2,-1,2]$\n",
    "\n",
    "$$\n",
    "||x|| = \\displaystyle\\sqrt{2^2 + (-1)^2 + 2^2} = \\sqrt{9} = 3\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1 Propieades de la norma"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como todas las operaciones con las que trabajamos hasta ahora la norma también cumple una serie de propiedades. Sean $x$, $y$ son $n$-vectores y $\\beta$ un escalar cualquiera, tenemos que la norma cumple:\n",
    "* Homogeneídad no negativa: $||\\beta x|| = |\\beta|\\cdot ||x||$\n",
    "\n",
    "* Desigualdad del triángulo: $||x+y|| \\le ||x|| + ||y||$ \n",
    "\n",
    "* No-negatividad: $||x||\\ge 0$ y además $||x|| = 0$ si y solamente si $x = \\mathbf{0}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La norma es ampliamente usada, en particular podemos señalar los siguientes ejemplos:\n",
    "\n",
    "**Valor cuadrático medio.** Algunas veces también es llamado simplemente **RMS** (root mean square) y se define como sigue:\n",
    "\n",
    "$$\n",
    "rms(x) = \\displaystyle\\sqrt{\\frac{x_{0}^2 + x_{1}^2 + \\cdots + x_{n-1}^{2}}{n}} = \\frac{||x||}{\\sqrt{n}}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Norma de una suma.** Si tenemos dos vectores $x$ y $y$ entonces la norma de la suma es\n",
    "\n",
    "$$\n",
    "||x+y|| = \\displaystyle\\sqrt{||x||^{2} + 2x^{T}y + ||y||^{2}}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Norma de una bloque de vectores.** La norma de bloque de vectores será simplemente la raíz cuadrada de la suma de sus componentes. Si tenemos $d$ un $k$-vector con componentes $d_{i}$ que son $n$-vectores entonces tenemos que:\n",
    "\n",
    "$$\n",
    "||d|| = \\displaystyle\\sqrt{||d_{0}||^{2}+||d_{1}||^{2}+\\cdots + ||d_{n-1}||^{2}}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Se importa la librerías necesarias\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Función para hallar la norma de un vectos\n",
    "def norma(x):\n",
    "    return np.sqrt(x@x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Se crea ek vector\n",
    "u = np.array([1,1])\n",
    "\n",
    "# Se halla la norma\n",
    "print('norma(u):\\n', norma(u))\n",
    "print('square_root(2):\\n', np.sqrt(2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "norma(u):\n",
      " 1.4142135623730951\n",
      "square_root(2):\n",
      " 1.4142135623730951\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Efectivamente la norma halla el valor de la raiz cuadrada de los elementos al cuadrado."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora se comprueba la propiedad de la homogeneidad no negativa con el vector u y un escalar de 2.\n",
    "\n",
    "$2*u^T = [2,2]$ entonces la norma será $||2u|| = \\displaystyle\\sqrt{2^2 + 2^2} = \\displaystyle\\sqrt{8}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Se comprueba la propiedad de la homogeneidad no negativa\n",
    "print('norma(2u):\\n', norma(2*u))\n",
    "print('2*norma(u):\\n', 2*norma(u))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "norma(2u):\n",
      " 2.8284271247461903\n",
      "2*norma(u):\n",
      " 2.8284271247461903\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "De esta forma se comprueba la propiedad de la homogeneidad no negativa. Ahora se comprueba la propiedad de la desigualdad del triangulo, el cual esta dada por:\n",
    "\n",
    "* Desigualdad del triángulo: $||x+y|| \\le ||x|| + ||y||$ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Se crea un nuevo vector\n",
    "v = np.array([-1,1])\n",
    "# Se comprueba la propiedad\n",
    "print('u:\\n', u)\n",
    "print('v:\\n', v)\n",
    "print('||u + v||:\\n', norma(u + v))\n",
    "print('¿Es verdad que ||u + v|| <= ||u|| + ||v||?: ', norma(u+v) <= norma(u) + norma(v))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "u:\n",
      " [1 1]\n",
      "v:\n",
      " [-1  1]\n",
      "||u + v||:\n",
      " 2.0\n",
      "¿Es verdad que ||u + v|| <= ||u|| + ||v||?:  True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se comprueba la propiedad de la desigualdad triangular."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CLASE 20: DISTANCIA ENTRE VECTORES"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1 Distancia entre vectores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos usar la norma pora definir la distancia entre dos vectores $\\mathbf{a}$ y $\\mathbf{b}$ como la norma de la diferencia de los vectores\n",
    "\n",
    "$$\n",
    "dist(\\mathbf{a},\\mathbf{b}) = ||\\mathbf{a} - \\mathbf{b}|| = \\displaystyle\\sqrt{(a_{0}-b_{0})^2 + (a_{1}-b_{1})^2 + \\cdots + (a_{n-1}-b_{n-1})^2}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Se escriben dos vectores\n",
    "a = np.array([1,1])\n",
    "b = np.array([-2,3])\n",
    "\n",
    "# Ahora se halla la resta de los dos vectores\n",
    "c = a - b\n",
    "\n",
    "print('a:\\n', a)\n",
    "print('b:\\n', b)\n",
    "print('c = a - b:\\n', c)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a:\n",
      " [1 1]\n",
      "b:\n",
      " [-2  3]\n",
      "c = a - b:\n",
      " [ 3 -2]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recordemos que sucede cuando se suman dos vectores\n",
    "\n",
    "![](imagenes/img7.png)\n",
    "\n",
    "El vector w es la suma de los vectores v + u, el cuál se puede formar por medio de la suma del paralelogramo (vectores en rojo)\n",
    "\n",
    "![](imagenes/img8.png)\n",
    "\n",
    "Ahora, si en ves de sumar a u le restamos el vector v, nos da como resultado el vector c, el cuál si lo trasladamos hasta el final del vector v, este nuevo vector d termina en donde termina el vector u, port tanto la norma de la resta de dos vectores da como resultado la distancia entre los dos vectores. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Distancia entre dos vectores\n",
    "print('distancia:\\n', norma(a - b))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "distancia:\n",
      " 3.605551275463989\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sin embargo numpy tiene la función para hallar la norma de un vector"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Función de python para hallar la norma\n",
    "print('np.linalg.norm(a - b):\\n', np.linalg.norm(a - b))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "np.linalg.norm(a - b):\n",
      " 3.605551275463989\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A esto se el llama _norma Euclidiana_ o *distancia Euclidiana* y tiene ese nombre porque es como se infiere la forma de medir distancias de los [postulados de Euclides](https://es.wikipedia.org/wiki/Postulados_de_Euclides). Ahora, la realidad es que siempre que nuestro espacio admita una $\\mathbf{L^p}$-norma entonces podremos generalizar la norma anterior como\n",
    "\n",
    "$$\n",
    "||x||_{p} = \\left(|x_{0}|^p + |x_{1}|^p + \\cdots + |x_{n-1}|^p\\right)^{1/p} \\qquad con \\;\\; 1\\le p\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por eso es que la norma Euclidiana también es conocida como la *norma $L_{2}$*. Hay otras normas que recibe nombres. Por ejemplo, si $p=1$ tenemos la norma $L_{1}$ y esta es llamada _la métrica del taxista_ o _la métrica de Manhattan_. Si tomamos $p \\to \\infty$ entonces tenemos $L_{\\infty}$ y esta es llamada _la métrica del máximo_. Es muy importante saber qué métrica estamos usando para medir distancias ya que la forma en la que medimos distancias está en total correspondencia en con configuración y la percepción el espacio. Por ejemplo, mientras que en la métrica Euclidiana la distancia corresponde a la más corta entre los dos vectores en el espacio planeo en el caso de la métrica del taxista es la más corta pero no será es segmento más directo sino por _bloques_."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para poder simplificar las cosas tomemos dos vectores $\\mathbf{x}$ y $\\mathbf{y}$ en $\\mathbb{R}^2$\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} 2\\\\1 \\end{bmatrix},\\; \\qquad \\mathbf{y} = \\begin{bmatrix} 1\\\\2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "La distancia en el caso de norma $L_{2}$ tenemos que la distancia entre los vectores es $||\\mathbf{x}-\\mathbf{y}||_{2} = \\displaystyle\\sqrt{(2-1)^{2} + (1-2)^{2}} = \\displaystyle\\sqrt{2}$ mientras que bajo la norma $L_{1}$ obtenemos que $||\\mathbf{x}-\\mathbf{y}||_{1} = |2-1|+|1-2| = 2$. \n",
    "\n",
    "Esto se puede representar graficamente como:\n",
    "\n",
    "![](imagenes/img9.png)\n",
    "\n",
    "Donde los vectores son u, v, la norma L2 esta representado en rojo mientras que la norma L1 esta representado en azul."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora, si $\\mathbf{x}$ y $\\mathbf{y}$ son $n$-vectores hay una medida quer después cobrará importancia, *la desviación cuadrática media*\n",
    "\n",
    "$$\n",
    "rms_{dev}(\\mathbf{x},\\mathbf{y}) = \\displaystyle\\frac{||\\mathbf{x}-\\mathbf{y}||}{\\sqrt{n}}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algo muy imporante es que a pesar de que medimos distancias no existen propiamente nociones de *cerca* y *lejos*. Si tomamos los siguientes $4$-vectores\n",
    "\n",
    "$$\n",
    "\\mathbf{u} = \\begin{bmatrix}1.8 \\\\ 2.8 \\\\ -3.7 \\\\ 4.7 \\end{bmatrix}, \\qquad  \\mathbf{v} = \\begin{bmatrix}0.6 \\\\ 2.1 \\\\ 1.9 \\\\ -1.4 \\end{bmatrix}, \\qquad \\mathbf{w} = \\begin{bmatrix}2.0 \\\\ 1.9 \\\\ -4.0 \\\\ 4.6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Tenemos que las distancia entre los vectores es:\n",
    "\n",
    "$$\n",
    "||\\mathbf{u}-\\mathbf{v}|| = 8.36, \\qquad ||\\mathbf{u}-\\mathbf{w}|| = 0.38, \\qquad ||\\mathbf{v}-\\mathbf{w}|| = 8.63\n",
    "$$\n",
    "\n",
    "Entonces así ya tenemos noción de cercanía o lontanaza, de no ser porque comparamos no tendríamos estas nociones. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Desigualdad del triángulo.** Debido a que la distancia la definimos a partir de la norma entonces la distancia también está sujeta a cumplir la desigualdad del triángulo. Considemos entonces tres $n$-vectores $\\mathbf{a}$,$\\mathbf{b}$ y $\\mathbf{c}$ entonces tenemos\n",
    "\n",
    "$$\n",
    "||\\mathbf{a}-\\mathbf{c}|| = ||\\mathbf{a}-\\mathbf{b}+\\mathbf{b}-\\mathbf{c}|| \\le ||\\mathbf{a}-\\mathbf{b}|| + ||\\mathbf{b}-\\mathbf{c}||\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a partir de los siguientes $2$-vectores:\n",
    "$$\n",
    "\\mathbf{a} = \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix}, \\qquad \\mathbf{b} = \\begin{bmatrix} 2\\\\ 3 \\end{bmatrix}, \\qquad \\mathbf{c} = \\begin{bmatrix} 4\\\\ 2 \\end{bmatrix}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enlaces Externos\n",
    "\n",
    "[Postulados de Euclides](https://es.wikipedia.org/wiki/Postulados_de_Euclides)\n",
    "\n",
    "[Colab Original](https://colab.research.google.com/drive/1pg6fxVEmEDlpou3JbEtnlZzcVckPGS8F?usp=sharing)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "402c3ce22046ee1ecb4526ff3875833594eb9f670609218e3d751b8e7c24a0d0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}